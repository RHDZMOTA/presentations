# End-to-End ML with Apache Spark

```{r setup-end-to-end-sparkml, include=FALSE}
reticulate::use_virtualenv(file.path(getwd(), 'venv'))
```

## Outline

1. ML Project Overview
2. Operationalizing ML
3. Spark-Based Projects
4. Minimal Architecture
5. Use case walk-through
6. Demo!

## Acknowledgments

This talk is based and inspired on the following conferences:

* add

Find the complete list of references in the **References** section.

## Prerequisites

This talk assumes you are a machine learning enthusiast or a data-professional (e.g. scientist, engineer) that is well aware the basic concepts required to design and execute an ML-project.

The audience must have a workable understanding of: 
* Main programming languages used in the data-sphere (i.e. [scala], [python], [R])
* General understanding of dataarchitecutres (e.g. batch-oriented, streaming)
* Machine Learning theory (i.e., lots of math).
* Machine Learning frameworks (e.g., [Spark ML], [Tensorflow], [PyTorch])
* Data-related skills (e.g., cleaning, visualization)

[scala]: https://www.scala-lang.org/
[python]: https://www.python.org/
[R]: https://www.r-project.org/
[Spark ML]: http://spark.apache.org
[Tensorflow]: https://www.tensorflow.org/
[PyTorch]: https://pytorch.org/

## ML Project Overview 


## Operationalizing

## Spark-Based Projects

### Intro to Spark ML

### Intro to Openscoring

## Minimal Architecture

## Problem definition

## Code Example!

### Problem definition

### Download the data

We can use the Gutenberg Project as a data-source for our ML task. To download the complete content of the gutenberg project as a set of txt-files run the following bash-command:

```bash
curl -sSL https://raw.githubusercontent.com/RHDZMOTA/spark-wordcount/develop/gutenberg.sh | sh
```

Depending on your network speed this can take up to 3 hours.

Let's figure out the "footprint" of this dataset:

* Number of books: `ls -l gutenberg | wc -l`
* Data size: `du -sh gutenberg`

Consider taking a random sample to facilitate local development. The following command generates a sample of 5K books:

```bash
mkdir gutenberg-sample && ls gutenberg/ | shuf -n 5000 | xargs -I _ cp gutenberg/_ gutenberg-sample/_
```

Place the resulting files into the `resources` directory of your project. 

### Minimum Setup

1. Install [Java 8] or greater.
    * Debain-based OS: `sudo apt install openjdk-8-jdk`
2. Install the [Scala Build Tool] (SBT)
    * Debian-based OS:

```bash
$ sudo apt install wget
$ wget https://dl.bintray.com/sbt/debian/sbt-1.2.6.deb
$ sudo dpkg -i sbt-1.2.6.deb
$ rm -r sbt-1.2.6.deb
$ sbt about
```

### WordCount

Let's do a quick wordcount example on the dataset as a warmup exercise:

```tut:book
import com.rhdzmota.presentations.Settings.S03
import com.rhdzmota.presentations.S03.config.Context
import org.apache.spark.sql._

object WordCount extends Context {
  import spark.implicits._

  final case class WordCount(word: String, count: Long)

  val data: Dataset[String] = spark.read.textFile(S03.Data.source)

  val wordcount: Dataset[WordCount] = data
    .flatMap(_.split("""\s+""")).map(_.toLowerCase.replaceAll("[^A-Za-z0-9]", "")).filter(_.length > 1)
    .groupByKey(identity).count().map({case (w, c) => WordCount(w, c)})
    .sort($"count".desc)

  def main(args: Array[String]): Unit = {
    println("S03 WordCount Application")
    wordcount.show()
    spark.close()
  }
}
```

Run:

```tut:book
WordCount.main(Array[String]())
```

Or:

```bash
sbt content/run
```


## References

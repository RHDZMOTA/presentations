<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>3 End-to-End ML with Apache Spark | Talks &amp; Presentations</title>
  <meta name="description" content="Let this book be a register of my talks and presentations.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="3 End-to-End ML with Apache Spark | Talks &amp; Presentations" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://rhdzmota.github.io/presentations/" />
  
  <meta property="og:description" content="Let this book be a register of my talks and presentations." />
  <meta name="github-repo" content="rhdzmota/presentations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 End-to-End ML with Apache Spark | Talks &amp; Presentations" />
  
  <meta name="twitter:description" content="Let this book be a register of my talks and presentations." />
  

<meta name="author" content="Rodrigo Hernández Mota">


<meta name="date" content="2019-01-25">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="monads-in-my-python.html">
<link rel="next" href="references-3.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i>How to read this book?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="what-is-this.html"><a href="what-is-this.html"><i class="fa fa-check"></i><b>1</b> What is this?</a><ul>
<li class="chapter" data-level="1.1" data-path="what-is-this.html"><a href="what-is-this.html#the-short-answer"><i class="fa fa-check"></i><b>1.1</b> The Short Answer</a></li>
<li class="chapter" data-level="1.2" data-path="what-is-this.html"><a href="what-is-this.html#the-long-answer"><i class="fa fa-check"></i><b>1.2</b> The Long Answer</a><ul>
<li class="chapter" data-level="1.2.1" data-path="what-is-this.html"><a href="what-is-this.html#motivation"><i class="fa fa-check"></i><b>1.2.1</b> Motivation</a></li>
<li class="chapter" data-level="1.2.2" data-path="what-is-this.html"><a href="what-is-this.html#the-proposed-approach"><i class="fa fa-check"></i><b>1.2.2</b> The proposed approach</a></li>
<li class="chapter" data-level="1.2.3" data-path="what-is-this.html"><a href="what-is-this.html#why-r-markdown"><i class="fa fa-check"></i><b>1.2.3</b> Why R markdown?</a></li>
<li class="chapter" data-level="1.2.4" data-path="what-is-this.html"><a href="what-is-this.html#publishing"><i class="fa fa-check"></i><b>1.2.4</b> Publishing</a></li>
<li class="chapter" data-level="1.2.5" data-path="what-is-this.html"><a href="what-is-this.html#presentations"><i class="fa fa-check"></i><b>1.2.5</b> Presentations</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="what-is-this.html"><a href="what-is-this.html#references"><i class="fa fa-check"></i><b>1.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="monads-in-my-python.html"><a href="monads-in-my-python.html"><i class="fa fa-check"></i><b>2</b> Monads in [My Py]thon</a><ul>
<li class="chapter" data-level="2.1" data-path="monads-in-my-python.html"><a href="monads-in-my-python.html#acknowledgments"><i class="fa fa-check"></i><b>2.1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2.2" data-path="monads-in-my-python.html"><a href="monads-in-my-python.html#about-mypy"><i class="fa fa-check"></i><b>2.2</b> About MyPy</a><ul>
<li class="chapter" data-level="2.2.1" data-path="monads-in-my-python.html"><a href="monads-in-my-python.html#why-types"><i class="fa fa-check"></i><b>2.2.1</b> Why types</a></li>
<li class="chapter" data-level="2.2.2" data-path="monads-in-my-python.html"><a href="monads-in-my-python.html#installation"><i class="fa fa-check"></i><b>2.2.2</b> Installation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="monads-in-my-python.html"><a href="monads-in-my-python.html#about-monads"><i class="fa fa-check"></i><b>2.3</b> About Monads</a></li>
<li class="chapter" data-level="2.4" data-path="monads-in-my-python.html"><a href="monads-in-my-python.html#monads-in-scala"><i class="fa fa-check"></i><b>2.4</b> Monads in Scala</a></li>
<li class="chapter" data-level="2.5" data-path="monads-in-my-python.html"><a href="monads-in-my-python.html#monads-in-python"><i class="fa fa-check"></i><b>2.5</b> Monads in python</a></li>
<li class="chapter" data-level="2.6" data-path="monads-in-my-python.html"><a href="monads-in-my-python.html#example"><i class="fa fa-check"></i><b>2.6</b> Example</a></li>
<li class="chapter" data-level="2.7" data-path="monads-in-my-python.html"><a href="monads-in-my-python.html#references-1"><i class="fa fa-check"></i><b>2.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html"><i class="fa fa-check"></i><b>3</b> End-to-End ML with Apache Spark</a><ul>
<li class="chapter" data-level="3.1" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#outline"><i class="fa fa-check"></i><b>3.1</b> Outline</a></li>
<li class="chapter" data-level="3.2" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#starting-questions"><i class="fa fa-check"></i><b>3.2</b> Starting Questions</a></li>
<li class="chapter" data-level="3.3" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#acknowledgments-1"><i class="fa fa-check"></i><b>3.3</b> Acknowledgments</a></li>
<li class="chapter" data-level="3.4" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#prerequisites"><i class="fa fa-check"></i><b>3.4</b> Prerequisites</a></li>
<li class="chapter" data-level="3.5" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#ml-project-overview"><i class="fa fa-check"></i><b>3.5</b> ML Project Overview</a><ul>
<li class="chapter" data-level="3.5.1" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#whats-a-ml-model"><i class="fa fa-check"></i><b>3.5.1</b> What’s a ML Model?</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#operationalizing"><i class="fa fa-check"></i><b>3.6</b> Operationalizing</a><ul>
<li class="chapter" data-level="3.6.1" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#traditional-approach"><i class="fa fa-check"></i><b>3.6.1</b> Traditional Approach</a></li>
<li class="chapter" data-level="3.6.2" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#a-simple-solution"><i class="fa fa-check"></i><b>3.6.2</b> A simple solution</a></li>
<li class="chapter" data-level="3.6.3" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#best-practice"><i class="fa fa-check"></i><b>3.6.3</b> Best Practice</a></li>
<li class="chapter" data-level="3.6.4" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#the-big-picture"><i class="fa fa-check"></i><b>3.6.4</b> The Big Picture</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#spark-based-projects"><i class="fa fa-check"></i><b>3.7</b> Spark-Based Projects</a><ul>
<li class="chapter" data-level="3.7.1" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#why-apache-spark"><i class="fa fa-check"></i><b>3.7.1</b> Why Apache Spark?</a></li>
<li class="chapter" data-level="3.7.2" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#intro-to-spark-ml"><i class="fa fa-check"></i><b>3.7.2</b> Intro to Spark ML</a></li>
<li class="chapter" data-level="3.7.3" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#intro-to-jpmml-and-openscoring"><i class="fa fa-check"></i><b>3.7.3</b> Intro to JPMML and Openscoring</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#code-example"><i class="fa fa-check"></i><b>3.8</b> Code Example!</a><ul>
<li class="chapter" data-level="3.8.1" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#download-the-data"><i class="fa fa-check"></i><b>3.8.1</b> Download the data</a></li>
<li class="chapter" data-level="3.8.2" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#minimum-setup"><i class="fa fa-check"></i><b>3.8.2</b> Minimum Setup</a></li>
<li class="chapter" data-level="3.8.3" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#wordcount"><i class="fa fa-check"></i><b>3.8.3</b> WordCount</a></li>
<li class="chapter" data-level="3.8.4" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#next-word-prediction"><i class="fa fa-check"></i><b>3.8.4</b> Next Word Prediction</a></li>
<li class="chapter" data-level="3.8.5" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#openscoring-container"><i class="fa fa-check"></i><b>3.8.5</b> Openscoring Container</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="end-to-end-ml-with-apache-spark.html"><a href="end-to-end-ml-with-apache-spark.html#references-2"><i class="fa fa-check"></i><b>3.9</b> References</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-3.html"><a href="references-3.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Talks &amp; Presentations</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="end-to-end-ml-with-apache-spark" class="section level1">
<h1><span class="header-section-number">3</span> End-to-End ML with Apache Spark</h1>
<div id="outline" class="section level2">
<h2><span class="header-section-number">3.1</span> Outline</h2>
<ol style="list-style-type: decimal">
<li>ML Project Overview</li>
<li>Operationalizing</li>
<li>Spark-Based Projects</li>
<li>Code Example!</li>
</ol>
</div>
<div id="starting-questions" class="section level2">
<h2><span class="header-section-number">3.2</span> Starting Questions</h2>
<ul>
<li>Who in here is a Data Professional (e.g., scientist, engineer)?</li>
<li>How many of you have used Apache Spark? in production?</li>
<li>How many of you are currently developing/maintaining a ML service?</li>
</ul>
</div>
<div id="acknowledgments-1" class="section level2">
<h2><span class="header-section-number">3.3</span> Acknowledgments</h2>
<p>This talk is based and inspired on the following conferences:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=SNxMLINtlbo">Operationalizing Machine Learning - Serving ML Models</a> by Boris Lublinsky</li>
<li><a href="https://www.youtube.com/watch?v=woRmeGvOaz4">Concept Drift: Monitoring Model Quality in Streaming Machine Learning Applications</a> by Emre Velipasaoglu</li>
<li><a href="https://www.youtube.com/watch?v=CdVXEQgmnfY">R, Scikit-Learn, and Apache Spark ML: What Difference Does It Make?</a> by Villu Ruusmann</li>
</ul>
<p>Find the complete list of references in the <strong>References</strong> section.</p>
</div>
<div id="prerequisites" class="section level2">
<h2><span class="header-section-number">3.4</span> Prerequisites</h2>
<p>This talk assumes you are a machine learning enthusiast or a data-professional (e.g. scientist, engineer) that is well aware the basic concepts required to design and execute an ML-project.</p>
<p>The audience must have a workable understanding of:</p>
<ul>
<li>Main programming languages used in the data-sphere (i.e. <a href="https://www.scala-lang.org/">scala</a>, <a href="https://www.python.org/">python</a>, <a href="https://www.r-project.org/">R</a>)</li>
<li>General understanding of data architecutres (e.g. batch-oriented, streaming)</li>
<li>Machine Learning theory (i.e., lots of math).</li>
<li>Machine Learning frameworks (e.g., <a href="http://spark.apache.org/mllib/">Spark ML</a>, <a href="https://www.tensorflow.org/">Tensorflow</a>, <a href="https://pytorch.org/">PyTorch</a>)</li>
<li>Data-related skills (e.g., cleaning, visualization)</li>
</ul>
</div>
<div id="ml-project-overview" class="section level2">
<h2><span class="header-section-number">3.5</span> ML Project Overview</h2>
<p>Typically with a ML Project, different groups are responsible for model training and serving. Moreover, the data science toolbox is constantly evolving, pushing software engineers to create more model-serving frameworks and introducing complexity to the development pipeline.</p>
<p>Consider the following machine-learning pipeline:</p>
<p><img src="../img/ml-cycle.png" alt="Machine Learning Cycle" /><br />
Machine Learning Cycle.</p>
<div id="whats-a-ml-model" class="section level3">
<h3><span class="header-section-number">3.5.1</span> What’s a ML Model?</h3>
<p>We will use the idea of a model as just a function <code>f</code> that transforms a set of inputs <code>x</code> into outputs <code>y</code> (i.e. <code>y = f(x)</code>).</p>
<p>This definition allows us to apply functional composition in the implementation of our ML service.</p>
<p>With this is mind, we can introduce the concept [machine learning pipelines ] as a graph defining a chain of operations (e.g., data transformations):</p>
<p><img src="../img/pipeline.png" alt="Machine Learning Pipeline" /><br />
Why is it important to define a pipeline? To encapsulate all the logic needed to serve the machine learning model. This formalizes the pipeline form the input data to the output.</p>
</div>
</div>
<div id="operationalizing" class="section level2">
<h2><span class="header-section-number">3.6</span> Operationalizing</h2>
<div id="traditional-approach" class="section level3">
<h3><span class="header-section-number">3.6.1</span> Traditional Approach</h3>
<p>Traditionally, the machine learning model was viewed as code. This code had to be somehow imported for serving in production.</p>
<p><img src="../img/impedance-mismatch.png" alt="Impedance Mismatch" /><br />
Impedance mismatch!</p>
</div>
<div id="a-simple-solution" class="section level3">
<h3><span class="header-section-number">3.6.2</span> A simple solution</h3>
<p>We can shift our thinking from a “code” perspective to a “data” perspective and represent the model using a standard specification that’s agnostic to the training process. We can use the PMML specification designed by the <a href="http://dmg.org/">Data Mining Group</a> to achieve this.</p>
<p>Predictive Markdown Model Language is:</p>
<blockquote>
<p>“an XML-based language that provides a way for applications to define statistical and data-mining models as well as to share models between PMML-compliant applications.”</p>
</blockquote>
<p><span class="citation">(Ruusmann <a href="#ref-pmml2017uusmann">2017</a>)</span></p>
<p>Integration with the most popular ML frameworks via JPMML:</p>
<ul>
<li><a href="https://github.com/jpmml/jpmml-sparkml">jpmml-sparkml</a></li>
<li><a href="https://github.com/jpmml/jpmml-sklearn">jpmml-sklearn</a></li>
<li><a href="https://github.com/jpmml/jpmml-r">jpmml-r</a></li>
<li><a href="https://github.com/jpmml/jpmml-xgboost">jpmml-xgboost</a></li>
<li><a href="https://github.com/jpmml/jpmml-tensorflow">jpmml-tensorflow</a></li>
</ul>
<p>Using these tools we can achieve:</p>
<p><img src="../img/simple-ml-scoring.png" alt="Simple Scoring" /><br />
Simple Scoring.</p>
</div>
<div id="best-practice" class="section level3">
<h3><span class="header-section-number">3.6.3</span> Best Practice</h3>
<p>We can use either a stream-processing engine (SPE e.g., Apache Spark, Flink) or a stream-processing library (SPL e.g., Akka Stream, Kafka Stream).</p>
<p><img src="../img/suggested-architecture.png" alt="Suggested Architecture" /><br />
Suggested architecture.</p>
<ul>
<li>SPE: Good fit for applications that require features provided out of the box by such engines.</li>
<li>SPL: Provide a programming model highly customizable and light-weight.</li>
</ul>
<p><span class="citation">(Lublinsky <a href="#ref-servingml2017boris">2017</a>)</span></p>
<p>We can use <a href="https://doc.akka.io/docs/akka/2.5/stream/">Akka Streams</a> - based on <a href="https://doc.akka.io/docs/akka/2.5/guide/tutorial_1.html">Akka Actors</a>, to implement the proposed architecture (see <a href="https://github.com/RHDZMOTA/cnap-policy-crawler">syntax example</a>). The result would look like this:</p>
<div class="figure">
<img src="../img/simple-akka-stream-ml.png" alt="Naive Akka Implementation" />
<p class="caption">Naive Akka Implementation</p>
</div>
<p>Simple Akka Implementation</p>
<p>Furthermore, we can enhance this approach by using Akka Clusters.</p>
<div class="figure">
<img src="../img/akka-cluster-ml.png" alt="Akka Cluster Implementation" />
<p class="caption">Akka Cluster Implementation</p>
</div>
<p>Akka Cluster Implementation</p>
</div>
<div id="the-big-picture" class="section level3">
<h3><span class="header-section-number">3.6.4</span> The Big Picture</h3>
<p>Dean Wampler does a fantastic job describing the overall picture of a data-driven system architecture.</p>
<p><img src="../img/complete-architecture.png" alt="Data Architecture" /><br />
Big Picture Architecture</p>
<p><span class="citation">(Wampler <a href="#ref-fastdata2017dean">2017</a>)</span></p>
</div>
</div>
<div id="spark-based-projects" class="section level2">
<h2><span class="header-section-number">3.7</span> Spark-Based Projects</h2>
<div id="why-apache-spark" class="section level3">
<h3><span class="header-section-number">3.7.1</span> Why Apache Spark?</h3>
<p>According to their website,</p>
<blockquote>
<p>“<a href="https://spark.apache.org/">Apache Spark</a> is a unified analytics engine for large-scale data processing.”</p>
</blockquote>
<p>According to the book “High Performance Spark - Best Practices for Scaling &amp; Optimizing Apache Spark”:</p>
<blockquote>
<p>“Apache Spark is a high-performance, general puropose distributed computer system. Spark enables us to process large quantities of data, beyond what can fit on a sinlge machine, with a high-level, relatively easy-to-use API. Uniquely, Spark allows us to write the logic of data transformations and machine learning algorithms in a way that is parallelizable, but relatively system agnostic.”</p>
</blockquote>
<p><span class="citation">(Karau and Warren <a href="#ref-karau2017high">2017</a>)</span></p>
<p>Most of the Apache Spark features revolve around a base data-structure called <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds">RDD</a>s (resilient distributed datasets). An RDD is a fault-tolerant collection of elements that can be operated on parallel.</p>
<p>Let’s initialize an Spark Session (sbt console: <code>sbt -Dscala.color &quot;content/console&quot;</code>):</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">import</span> org.<span class="fu">apache</span>.<span class="fu">spark</span>.<span class="fu">sql</span>.<span class="fu">SparkSession</span>

<span class="kw">val</span> spark = 
  SparkSession.<span class="fu">builder</span>.<span class="fu">appName</span>(<span class="st">&quot;Example!&quot;</span>).<span class="fu">config</span>(<span class="st">&quot;spark.master&quot;</span>, <span class="st">&quot;local[*]&quot;</span>).<span class="fu">getOrCreate</span>()

<span class="kw">import</span> spark.<span class="fu">implicits</span>._</code></pre></div>
<p>By default, the number of partitions is the number of all available cores <span class="citation">(Laskowski <a href="#ref-laskowski2017mastering">2017</a>)</span>:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala">spark.<span class="fu">sparkContext</span>.<span class="fu">defaultParallelism</span>
<span class="co">// res0: Int = 12</span></code></pre></div>
<p>We can test this by creating a simple Dataset from a list:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala">
<span class="kw">trait</span> Person

<span class="kw">object</span> Person {
  <span class="kw">final</span> <span class="kw">case</span> <span class="kw">class</span> <span class="fu">Dead</span>(name: String, birthYear: Int, deadYear: Int) <span class="kw">extends</span> Person {
      <span class="kw">def</span> kill: Dead = <span class="kw">this</span>
    }

  <span class="kw">final</span> <span class="kw">case</span> <span class="kw">class</span> <span class="fu">Alive</span>(name: String, birthYear: Int) <span class="kw">extends</span> Person {
    <span class="kw">def</span> kill: Dead = <span class="fu">Dead</span>(name, birthYear, <span class="dv">2019</span>)
  }
  
  <span class="kw">val</span> names: List[String] = List(
    <span class="st">&quot;Data Ninja&quot;</span>,
    <span class="st">&quot;Random Developer&quot;</span>,
    <span class="st">&quot;Pizza Lover&quot;</span>,
    <span class="st">&quot;Beer Lover&quot;</span>
  )

  <span class="kw">val</span> years: List[Int] = (<span class="dv">1980</span> to <span class="dv">2000</span>).<span class="fu">toList</span>

  <span class="kw">def</span> getRandomElement[A](ls: List[A]): A = 
    <span class="fu">ls</span>(scala.<span class="fu">util</span>.<span class="fu">Random</span>.<span class="fu">nextInt</span>(ls.<span class="fu">size</span>))

  <span class="kw">def</span> getRandom: Alive = <span class="fu">Alive</span>(<span class="fu">getRandomElement</span>(names), <span class="fu">getRandomElement</span>(years))
}

<span class="kw">val</span> people: List[Person.<span class="fu">Alive</span>] = (<span class="dv">1</span> to <span class="dv">1000</span>).<span class="fu">toList</span>.<span class="fu">map</span>(i =&gt; Person.<span class="fu">getRandom</span>)</code></pre></div>
<p>We can now create a <code>Dataset[Person]</code>:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">import</span> org.<span class="fu">apache</span>.<span class="fu">spark</span>.<span class="fu">sql</span>.<span class="fu">Dataset</span>

<span class="kw">val</span> alivePeople: Dataset[Person.<span class="fu">Alive</span>] = spark.<span class="fu">createDataset</span>(people)</code></pre></div>
<p>The number of partitions on this dataset:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala">alivePeople.<span class="fu">rdd</span>.<span class="fu">partitions</span>.<span class="fu">size</span>
<span class="co">// res1: Int = 12</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> deadPeople: Dataset[Person.<span class="fu">Dead</span>] = 
  alivePeople.<span class="fu">filter</span>(_.<span class="fu">birthYear</span> &gt; <span class="dv">1994</span>).<span class="fu">map</span>(person =&gt; person.<span class="fu">kill</span>)
<span class="co">// deadPeople: org.apache.spark.sql.Dataset[Person.Dead] = [name: string, birthYear: int ... 1 more field]</span>

deadPeople.<span class="fu">show</span>()
<span class="co">// +----------------+---------+--------+</span>
<span class="co">// |            name|birthYear|deadYear|</span>
<span class="co">// +----------------+---------+--------+</span>
<span class="co">// |      Data Ninja|     1998|    2019|</span>
<span class="co">// |Random Developer|     2000|    2019|</span>
<span class="co">// |Random Developer|     1996|    2019|</span>
<span class="co">// |      Data Ninja|     1995|    2019|</span>
<span class="co">// |Random Developer|     1996|    2019|</span>
<span class="co">// |      Beer Lover|     1997|    2019|</span>
<span class="co">// |      Beer Lover|     1997|    2019|</span>
<span class="co">// |     Pizza Lover|     1997|    2019|</span>
<span class="co">// |      Data Ninja|     1999|    2019|</span>
<span class="co">// |      Beer Lover|     1997|    2019|</span>
<span class="co">// |      Data Ninja|     1996|    2019|</span>
<span class="co">// |      Beer Lover|     1996|    2019|</span>
<span class="co">// |      Beer Lover|     1997|    2019|</span>
<span class="co">// |     Pizza Lover|     1997|    2019|</span>
<span class="co">// |      Data Ninja|     1995|    2019|</span>
<span class="co">// |      Beer Lover|     1999|    2019|</span>
<span class="co">// |Random Developer|     1995|    2019|</span>
<span class="co">// |     Pizza Lover|     1995|    2019|</span>
<span class="co">// |      Data Ninja|     1997|    2019|</span>
<span class="co">// |     Pizza Lover|     2000|    2019|</span>
<span class="co">// +----------------+---------+--------+</span>
<span class="co">// only showing top 20 rows</span>
<span class="co">// </span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala">spark.<span class="fu">close</span>()</code></pre></div>
<p>For performance reasons, this presentation will use the official Scala API.</p>
</div>
<div id="intro-to-spark-ml" class="section level3">
<h3><span class="header-section-number">3.7.2</span> Intro to Spark ML</h3>
<p>Spark ML is a practical and scalable machine learning library based on a [Dataset]. A Dataset is a distributed collection of data with interesting features such as strong typing, lambda functions, and with the advantages of the Spark SQL’s optimized execution engine. We can manipulate a dataset with functional transformantions. The most basic ones:</p>
<ul>
<li>map - <code>Dataset[A].map(fn: A =&gt; B): Dataset[B]</code></li>
<li>flatMap - <code>Dataset[A].flatMap(fn: A =&gt; Dataset[B]): Dataset[B]</code></li>
<li>filter - <code>Dataset[A].filter(fn: A =&gt; Boolean): Dataset[A]</code></li>
</ul>
<p>One of the most usefull abstractions available on the Spark ML package are pipelines. Main concepts:</p>
<ul>
<li><code>Dataset[Row]</code>: A set of data, also called dataframe. Each row usually represents an observation.</li>
<li><code>Transformer</code>: an algorithm that takes one <code>DataFrame</code> and returns another <code>DataFrame</code>.</li>
<li><code>Estimator</code>: an algorithm that takes a <code>DataFrame</code> and returns a <code>Transformer</code>.</li>
<li><code>Pipeline</code>: a chain of multiple <code>Transformer</code> or <code>Estimator</code>.</li>
</ul>
</div>
<div id="intro-to-jpmml-and-openscoring" class="section level3">
<h3><span class="header-section-number">3.7.3</span> Intro to JPMML and Openscoring</h3>
<p>Data Scientist might use Python and R for exploration and modeling while software engineers use Scala, Java, or Go for the system architecture. Complexity arises when dealing with multiple runtimes and trying to integrate the data solutions into the system. One way to standardize this interaction is via PMML: Predictive Markdown Model Language.</p>
<p>To use the jpmml-sparkml library, just add the following dependency to your sbt file:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="st">&quot;org.jpmml&quot;</span> % <span class="st">&quot;jpmml-sparkml&quot;</span> % <span class="st">&quot;1.4.5&quot;</span></code></pre></div>
<p>Now we can just take a Spark <code>PipelineModel</code> and create a PMML object:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> pmmlBuilder = <span class="kw">new</span> <span class="fu">PMMLBuilder</span>(schema, pipelineModel)
pmmlBuilder.<span class="fu">build</span>()</code></pre></div>
<p>See the official <a href="https://github.com/jpmml/jpmml-sparkml">jpmml-sparkml github repo</a> for a complete list of supported <code>PipelineStages</code> types.</p>
<p>We can use <a href="https://github.com/openscoring/openscoring">Openscoring</a>, a java-based REST web-service, as our scoring-engine of the resulting PMML model.</p>
<ul>
<li>Simple but powerful API</li>
<li>Allows for single predictions and for batch predictions.</li>
<li>Acceptable performance (usually sub-milliseconds respond time)</li>
</ul>
<p>Model REST API endpoints:</p>
<table>
<thead>
<tr class="header">
<th>HTTP method</th>
<th>Endpoint</th>
<th>Required role(s)</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GET</td>
<td>/model</td>
<td>-</td>
<td>Get the summaries of all models</td>
</tr>
<tr class="even">
<td>PUT</td>
<td>/model/${id}</td>
<td>admin</td>
<td>Deploy a model</td>
</tr>
<tr class="odd">
<td>GET</td>
<td>/model/${id}</td>
<td>-</td>
<td>Get the summary of a model</td>
</tr>
<tr class="even">
<td>GET</td>
<td>/model/${id}/pmml</td>
<td>admin</td>
<td>Download a model as a PMML document</td>
</tr>
<tr class="odd">
<td>POST</td>
<td>/model/${id}</td>
<td>-</td>
<td>Evaluate data in “single prediction” mode</td>
</tr>
<tr class="even">
<td>POST</td>
<td>/model/${id}/batch</td>
<td>-</td>
<td>Evaluate data in “batch prediction” mode</td>
</tr>
<tr class="odd">
<td>POST</td>
<td>/model/${id}/csv</td>
<td>-</td>
<td>Evaluate data in “CSV prediction” mode</td>
</tr>
<tr class="even">
<td>DELETE</td>
<td>/model/${id}</td>
<td>admin</td>
<td>Undeploy a model</td>
</tr>
</tbody>
</table>
<p><img src="../img/ml-complexity-vs-size.png" alt="ML Complexity VS Dataset Size" /><br />
Complexity vs dataset size.</p>
<p><span class="citation">(Ruusmann <a href="#ref-pmml2017uusmann">2017</a>)</span></p>
</div>
</div>
<div id="code-example" class="section level2">
<h2><span class="header-section-number">3.8</span> Code Example!</h2>
<div id="download-the-data" class="section level3">
<h3><span class="header-section-number">3.8.1</span> Download the data</h3>
<p>We can use the Gutenberg Project as a data-source for our ML task. To download the complete content of the gutenberg project as a set of txt-files run the following bash-command:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">curl</span> -sSL https://raw.githubusercontent.com/RHDZMOTA/spark-wordcount/develop/gutenberg.sh <span class="kw">|</span> <span class="fu">sh</span></code></pre></div>
<p>Depending on your network speed this can take up to 3 hours.</p>
<p>Let’s figure out the “footprint” of this dataset:</p>
<ul>
<li>Number of books: <code>ls -l gutenberg | wc -l</code></li>
<li>Data size: <code>du -sh gutenberg</code></li>
</ul>
<p>Consider taking a random sample to facilitate local development. The following command generates a sample of 5K books:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="fu">mkdir</span> gutenberg-sample <span class="kw">&amp;&amp;</span> <span class="fu">ls</span> gutenberg/ <span class="kw">|</span> <span class="ex">shuf</span> -n 5000 <span class="kw">|</span> <span class="fu">xargs</span> -I _ cp gutenberg/_ gutenberg-sample/_</code></pre></div>
<p>Printing the results.</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="bu">echo</span> <span class="st">&quot;There are </span><span class="va">$(</span><span class="fu">ls</span> -l gutenberg <span class="kw">|</span> <span class="fu">wc</span> -l<span class="va">)</span><span class="st"> books that represent: </span><span class="va">$(</span><span class="fu">du</span> -sh gutenberg<span class="va">)</span><span class="st">&quot;</span></code></pre></div>
</div>
<div id="minimum-setup" class="section level3">
<h3><span class="header-section-number">3.8.2</span> Minimum Setup</h3>
<ol style="list-style-type: decimal">
<li>Install [Java 8] or greater.
<ul>
<li>Debain-based OS: <code>sudo apt install openjdk-8-jdk</code></li>
</ul></li>
<li>Install the [Scala Build Tool] (SBT)
<ul>
<li>Debian-based OS:</li>
</ul></li>
</ol>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">$ <span class="fu">sudo</span> apt install wget
$ <span class="fu">wget</span> https://dl.bintray.com/sbt/debian/sbt-1.2.6.deb
$ <span class="fu">sudo</span> dpkg -i sbt-1.2.6.deb
$ <span class="fu">rm</span> -r sbt-1.2.6.deb
$ <span class="ex">sbt</span> about</code></pre></div>
</div>
<div id="wordcount" class="section level3">
<h3><span class="header-section-number">3.8.3</span> WordCount</h3>
<p>Let’s do a quick wordcount example on the dataset as a warm-up exercise:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">import</span> com.<span class="fu">rhdzmota</span>.<span class="fu">presentations</span>.<span class="fu">Settings</span>.<span class="fu">S03</span>
<span class="kw">import</span> com.<span class="fu">rhdzmota</span>.<span class="fu">presentations</span>.<span class="fu">S03</span>.<span class="fu">config</span>.<span class="fu">Context</span>
<span class="kw">import</span> org.<span class="fu">apache</span>.<span class="fu">spark</span>.<span class="fu">sql</span>._

<span class="kw">object</span> WordCount <span class="kw">extends</span> Context {
  <span class="kw">import</span> spark.<span class="fu">implicits</span>._

  <span class="kw">final</span> <span class="kw">case</span> <span class="kw">class</span> <span class="fu">WordCount</span>(word: String, count: Long)

  <span class="kw">val</span> data: Dataset[String] = spark.<span class="fu">read</span>.<span class="fu">textFile</span>(S03.<span class="fu">Data</span>.<span class="fu">source</span>)

  <span class="kw">val</span> wordcount: Dataset[WordCount] = data
    .<span class="fu">flatMap</span>(_.<span class="fu">split</span>(<span class="st">&quot;&quot;&quot;\s+&quot;&quot;&quot;</span>)).<span class="fu">map</span>(_.<span class="fu">toLowerCase</span>.<span class="fu">replaceAll</span>(<span class="st">&quot;[^A-Za-z0-9]&quot;</span>, <span class="st">&quot;&quot;</span>)).<span class="fu">filter</span>(_.<span class="fu">length</span> &gt; <span class="dv">1</span>)
    .<span class="fu">groupByKey</span>(identity).<span class="fu">count</span>().<span class="fu">map</span>({<span class="kw">case</span> (w, c) =&gt; <span class="fu">WordCount</span>(w, c)})
    .<span class="fu">sort</span>($<span class="st">&quot;count&quot;</span>.<span class="fu">desc</span>)

  <span class="kw">def</span> <span class="fu">main</span>(args: Array[String]): Unit = {
    <span class="fu">println</span>(<span class="st">&quot;S03 WordCount Application&quot;</span>)
    wordcount.<span class="fu">show</span>()
    spark.<span class="fu">close</span>()
  }
}</code></pre></div>
<p>Run:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala">WordCount.<span class="fu">main</span>(Array[String]())</code></pre></div>
<p>Or:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">sbt</span> <span class="st">&quot;content/runMain com.rhdzmota.presentations.S03.WordCount&quot;</span> </code></pre></div>
</div>
<div id="next-word-prediction" class="section level3">
<h3><span class="header-section-number">3.8.4</span> Next Word Prediction</h3>
<p>The challenge we have consists con taking an n-set of books and create a model that’s capable of predicting the next word given a context of the last m-words.</p>
<p>A similar approach is performed for generating <a href="https://www.tensorflow.org/tutorials/representation/word2vec">word embeddings</a> based on the distributional hypothesis - words that appear in the same contexts share semantic meaning.</p>
</div>
<div id="openscoring-container" class="section level3">
<h3><span class="header-section-number">3.8.5</span> Openscoring Container</h3>
<p>We can easily leverage Openscoring with Docker.</p>
<p>Consider the following <code>Dockerfile</code>:</p>
<pre class="docker"><code>FROM maven:3.5-jdk-8-alpine

RUN apk update &amp;&amp; apk upgrade &amp;&amp; apk add --no-cache bash ca-certificates wget openssh

RUN wget https://github.com/openscoring/openscoring/releases/download/1.4.3/openscoring-server-executable-1.4.3.jar

ADD application.conf application.conf

ENTRYPOINT java -Dconfig.file=application.conf -jar openscoring-server-executable-1.4.3.jar

EXPOSE 8080

CMD []
</code></pre>
<p>And the following <code>application.conf</code> file:</p>
<pre class="text"><code>application {
  // List of JAX-RS Component class names that must be registered
  componentClasses = [
    &quot;org.openscoring.service.filters.NetworkSecurityContextFilter&quot;,
    &quot;org.openscoring.service.filters.ServiceIdentificationFilter&quot;
  ]
}

networkSecurityContextFilter {
  // List of trusted IP addresses. An empty list defaults to all local network IP addresses.
  // A client that originates from a trusted IP address (as indicated by the value of the CGI variable REMOTE_ADDR) is granted the &quot;admin&quot; role.
  trustedAddresses = [&quot;*&quot;]
}</code></pre>
<p>We can create our custom image with:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">docker</span> build -t next-word-demo/openscoring resources/provided/docker/</code></pre></div>
<p>Now we can run a docker container with:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">docker</span> run -p 8080:8080 -d --name next-word-engine next-word-demo/openscoring</code></pre></div>
<p>You can test this service is running by going to: <code>http://{ip-address}:8080/openscoring</code> where <code>ip-address</code> is your <code>docker-machine ip</code> or <code>localhost</code>. We can now upload the resulting dataset to the Openscoring API:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">curl</span> -X PUT --data-binary @resources/output/model/2019-01-25T01-07-14.836-89d19488-3d3d-483c-a2f0-47caf685d7db-96.pmml \
-H <span class="st">&quot;Content-type: text/xml&quot;</span> \
http://192.168.99.100:8080/openscoring/model/next-word</code></pre></div>
<p>We should see the model in <code>http://{ip-address}:8080/openscoring/model/next-word-demo</code>.</p>
<p>Enjoy your scoring!</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">curl</span> -X POST --data-binary @resources/provided/requests/req-01.json \
    -H <span class="st">&quot;Content-type: application/json&quot;</span> \
    http://192.168.99.100:8080/openscoring/model/next-word \
    <span class="kw">|</span> <span class="ex">jq</span> <span class="st">&#39;.result.&quot;pmml(prediction)&quot;&#39;</span></code></pre></div>
</div>
</div>
<div id="references-2" class="section level2">
<h2><span class="header-section-number">3.9</span> References</h2>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-pmml2017uusmann">
<p>Ruusmann, Villu. 2017. “R, Scikit-Learn, and Apache Spark Ml: What Difference Does It Make?” Youtube - StartApp. <a href="https://www.youtube.com/watch?v=CdVXEQgmnfY" class="uri">https://www.youtube.com/watch?v=CdVXEQgmnfY</a>.</p>
</div>
<div id="ref-servingml2017boris">
<p>Lublinsky, Boris. 2017. <em>Serving Machine Learning Models - a Guide to Architecture, Stream, Processing Engines, and Frameworks</em>. “O’Reilly Media, Inc.”</p>
</div>
<div id="ref-fastdata2017dean">
<p>Wampler, Dean. 2017. “Fast Data Architectures for Streaming Applications.” Youtube - GOTO Conferences. <a href="https://www.youtube.com/watch?v=oCW5y4_8uGU" class="uri">https://www.youtube.com/watch?v=oCW5y4_8uGU</a>.</p>
</div>
<div id="ref-karau2017high">
<p>Karau, Holden, and Rachel Warren. 2017. <em>High Performance Spark: Best Practices for Scaling and Optimizing Apache Spark</em>. “ O’Reilly Media, Inc.”</p>
</div>
<div id="ref-laskowski2017mastering">
<p>Laskowski, Jacek. 2017. “Mastering Apache Spark.” <em>Gitbook: Https://Jaceklaskowski.gitbooks.io/Mastering-Apache-Spark</em> 25.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="monads-in-my-python.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references-3.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
